# Feed-Forward Neural Network (from scratch, NumPy)

A compact, educational implementation of a **multilayer perceptron (MLP)** trained with backpropagation and gradient-based optimization â€” using **only NumPy** (no high-level DL frameworks).

## Features
- Dense feed-forward layers with configurable `layer_dims`
- Sigmoid/ReLU activations (extendable)
- Mini-batch training, L2 regularization, dropout (if enabled)
- Metrics & simple plotting utilities (loss/accuracy)
- Logistic regression implemented separately

## Examples

See the Jupyter Notebook for a simple run on dataset generated with sklearn
